\documentclass[a4paper, 12pt]{article}

\usepackage[a4paper,bindingoffset=0.2in,%
            left=0.5in,right=0.7in,top=0.7in,bottom=1.25in,%
            footskip=0.5in]{geometry}
\usepackage{amsmath, amssymb, amsfonts, amsthm, mathtools}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{float}
\usepackage{slashbox}

\setlength{\parindent}{0pt}

\title{
	{\huge\bf Summer of Science 2020} \\
	[1cm]
	\textit{Final Report} \\
	[2cm]
	{\LARGE\scshape An Overview of Machine Learning,\\Neural Networks, and Deep Learning} \\
	[2cm]
}
\author{
	Ankit Kumar Misra \\
	Roll No: 190050020 \\
	Computer Science and Engineering \\
	IIT Bombay \\
	[2cm]
	Mentor: Soumya Chatterjee \\
	[2cm]
}
\date{\today}

\begin{document}

\pagenumbering{gobble}

\maketitle

\newpage

\tableofcontents

\newpage

\pagenumbering{arabic}

\topskip0pt
\vspace*{\fill}
\begin{center}
{\huge\scshape Part I \\\vspace{1cm} Machine Learning}
\end{center}
\vspace*{\fill}

\newpage

\section{What is Machine Learning?}
The term \textit{machine learning} was coined by Arthur Samuel in 1959. He defined it as the field of study that gives computers the ability to learn without being explicitly programmed. \\
\break
In 1998, Tom Mitchell came up with a much more formal definition of a machine learning algorithm: \\
\textit{A computer program is said to learn from an experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.} \\
\break
The main categories of machine learning algorithms are:
\begin{enumerate}
\item Supervised Learning
\item Unsupervised Learning
\item Reinforcement Learning
\end{enumerate}

\subsection{Supervised Learning}
Supervised learning algorithms are used when each training example in the training data consists of input feature values as well as the corresponding desired outputs. Thus, the algorithm is given the 'right answers' for the training inputs. Using a predefined cost function, the algorithm tries to generate a function that best fits this training data, and later uses this function to predict outputs for unseen inputs, that were not part of the training data. Supervised learning problems are of two types:
\begin{enumerate}
\item \textit{Regression}: These problems require the machine learning model to predict a continuous valued output; e.g., a housing price prediction problem is a regression task, since prices can take continuous values.
\item \textit{Classification}: These problems require the machine learning model to predict a discrete valued output; e.g., a problem which requires us to predict whether certain cancer tumors are benign or malignant is a classification task, since the output can take only two values; a tumor is either benign or malignant.
\end{enumerate}

\subsection{Unsupervised Learning}
Unsupervised learning algorithms are used to find structure in unlabeled and unclassified datasets that contains only inputs. The algorithm clusters data points; i.e., it assigns a set of observations into subsets (clusters) so that observations within the same cluster is similar, while observations from different clusters are dissimilar. Unsupervised learning algorithms are used in applications such as grouping similar news articles from different sources, social media analysis, market segmentation, and even astronomical data analysis.

\subsection{Reinforcement Learning}
Reinforcement learning algorithms are concerned with what actions a software agent should take in an environment in order to maximize a notion of cumulative reward. Owing to this generality, reinforcement learning algorithms are used in widely varying fields. The environment is represented as a Markov Decision Process, and the algorithm tries to take decisions based on its previous experiences in similar environments, thus attempting to maximize its reward.


\section{Linear Regression}
Linear regression is a supervised machine learning regression algorithm which, given a training dataset along with the corresponding continuous valued labels, determines the straight line function which best fits the training data. Thus, it assumes a linear function for the task and optimizes the function's parameters to fit the training data.
\vspace{1cm}
\hrule
\begin{center}
\textbf{NOTATION} \\
[8mm]
\begin{tabular}{p{1.8cm}p{9cm}}
$m$ & the number of instances in the dataset\\
[4mm]
$\bm{x^{\left(i\right)}}$ & the vector of all feature values (excluding the label) of the $i^{th}$ instance in the dataset\\
[4mm]
$y^{\left(i\right)}$ & the label (desired/expected output value) of the $i^{th}$ instance in the dataset \\
[4mm]
$\bm{x_j}$ & the vector of values from all data instances corresponding to the $j^{th}$ feature in the dataset\\
[4mm]
$x^{\left(i\right)}_j$ & the value of the $j^{th}$ feature in the $i^{th}$ instance of the dataset \\
[4mm]
$\bm{X}$ & the entire dataset (excluding labels) \\
[4mm]
$\bm{y}$ & the vector of all labels for the dataset \\
[4mm]
${\theta}_j$ & the parameter of the regression model which corresponds to the $j^{th}$ feature in the dataset \\
[4mm]
$\bm{\theta}$ & the vector of all parameters of the linear regression model \\
[4mm]
$h_{\theta}\left(\bm{x^{\left(i\right)}}\right)$ & the predicted label (output value) for the $i^{th}$ training instance
\end{tabular}
\end{center}
\hrule

\subsection{Hypothesis Function}
In linear regression tasks, the hypothesis function for an input data instance $\bm{x}$ having $n$ features and parameter vector $\bm{\theta}$ is given by:
\begin{align*}
h_{\theta}\left(\bm{x}\right) = \bm{\theta}^T\bm{x} = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n
\end{align*}
Here, $x_0$ is always equal to 1, and $\theta_0$ is called the \textit{bias term}. It can be thought of as the intercept parameter. Therefore, any linear regression task is effectively a problem of analyzing the given training dataset and figuring out the best possible parameter vector which achieves the maximum accuracy among all possible parameter vectors. This is where the cost function comes in.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{linear_regression}
\caption{Linear regression}
\end{figure}

\subsection{Cost Function}
In a machine learning algorithm, the cost function is used to determine the most accurate set of parameters which fits the training dataset. It provides a measure of the inaccuracy of the machine learning model, and the best possible parameters are determined by minimizing the cost function, using various methods. In linear regression tasks, the most commonly used cost function is:
\begin{align*}
J\left(\bm{\theta}\right) = \dfrac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(\bm{x^{\left(i\right)}}\right)-y^{\left(i\right)}\right)^2 = \dfrac{1}{2m}\sum_{i=1}^{m}\left(\bm{\theta}^T\bm{x^{\left(i\right)}}-y^{\left(i\right)}\right)^2
\end{align*}
The linear regression model attempts to optimize this cost function, i.e., it calculates the parameters that minimize it. Several methods are available for this optimization, such as batch gradient descent, stochastic gradient descent, mini-batch gradient descent, and the method of normal equations.

\subsection{Gradient Descent}
The method of gradient descent first assumes a totally random parameter vector, and then moves towards the optimal parameters step by step. During each such step, the gradient of the cost function at the current parameter vector is computed, and a multiple of the gradient is subtracted from the parameter vector, so as to always keep moving along the direction of fastest descent. Note that we have to be careful with this method, since gradient descent moves towards a local minimum, and may never reach the global minimum.
The mathematical formulation for each step of gradient descent is as follows:
\begin{align*}
\bm{\theta}_{\text{new}} = \bm{\theta} - \alpha
\begin{pmatrix}
\dfrac{\partial}{\partial \theta_0}J\left(\bm{\theta}\right) \\
\dfrac{\partial}{\partial \theta_1}J\left(\bm{\theta}\right) \\
\vdots \\
\dfrac{\partial}{\partial \theta_n}J\left(\bm{\theta}\right) \\
\end{pmatrix}
= \bm{\theta} - \dfrac{\alpha}{m}
\begin{pmatrix}
\sum_{i=1}^{m}\left(h_{\theta}\left(\bm{x^{\left(i\right)}}\right)-y^{\left(i\right)}\right)x_0^{\left(i\right)} \\
\sum_{i=1}^{m}\left(h_{\theta}\left(\bm{x^{\left(i\right)}}\right)-y^{\left(i\right)}\right)x_1^{\left(i\right)} \\
\vdots \\
\sum_{i=1}^{m}\left(h_{\theta}\left(\bm{x^{\left(i\right)}}\right)-y^{\left(i\right)}\right)x_n^{\left(i\right)} \\
\end{pmatrix}
\end{align*}
\begin{align*}
\implies \bm{\theta}_{\text{new}} = \bm{\theta} - \dfrac{\alpha}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(\bm{x^{\left(i\right)}}\right)-y^{\left(i\right)}\right)\bm{x^{\left(i\right)}} = \bm{\theta} - \dfrac{\alpha}{m}\bm{X}^T\left(\bm{X\theta} - \bm{y}\right)
\end{align*}
This formula is used for a method called batch gradient descent, where the entire dataset is considered in every single iteration, thus slowing down the training. To speed up the process, we can use stochastic gradient descent (one randomly selected instance in every iteration) or mini-batch gradient descent (a randomly selected subset of the dataset in every iteration).
\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{gradient_descent}
\caption{Movement towards lower cost function values using gradient descent. This figure shows a dataset consisting of only two features, with corresponding parameters $\theta_0$ and $\theta_1$.}
\end{figure}

\subsection{Normal Equations}
The normal equations method gives us a direct formula to calculate the optimized vector of parameters:
\begin{align*}
\bm{\theta} = \left(\bm{X}^T\bm{X}\right)^{-1}\bm{X}^T\bm{y}
\end{align*}
Although this method gives us a direct method to obtain the answer in just one step, there is one major problem; the computation of the inverse of $\bm{X}^T\bm{X}$. This matrix is a square matrix having order equal to the number of instances in the dataset, and the inverse calculation becomes computationally very heavy for larger datasets. The complexity of matrix inversion is $O(n^3)$, and this is a huge drawback for this method. Thus, gradient descent is preferred for datasets with more than $10,000$ data instances, while the normal equations method is used for smaller datasets.

\subsection{Polynomial Regression}
Although polynomial regression is different from linear regression, a polynomial regression task can be reduced to a linear regression task by simply adding more features, formed by multiplication of powers of the existing features (according to the degree of polynomial regression), and then applying ordinary linear regression. Some datasets are better fit by polynomial functions than linear functions, and thus, a higher degree polynomial is often a better solution for the problem.
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{polynomial_regression}
\caption{Polynomial regression}
\end{figure}


\section{Logistic Regression}
Logistic regression is (quite confusingly) a classification algorithm, under supervised learning. Given a training dataset along with the corresponding discrete valued labels, it determines a function to fit the dataset. The functioning of this algorithm is similar to that of linear regression. In a binary classification task, there is a scalar label which is either 0 or 1, whereas in multilabel classification, the label can be expressed as a vector where the index equal to the correct class number has value 1 and all others have value 0. We discuss binary classification first.

\subsection{Hypothesis Function}
The hypothesis function in this case forces the weighted sum of feature values into the interval $\left[0, 1\right]$. This number represents the predicted probability of that instance having a label of 1. The hypothesis function is usually written as:
\begin{align*}
h_{\theta}\left(\bm{x}\right) = g\left(\bm{\theta}^T\bm{x}\right) \\
\text{where } \hspace{2mm} g\left(z\right) = \dfrac{1}{1+e^{-z}} \\
\therefore \hspace{2mm} h_{\theta}\left(\bm{x}\right) = \dfrac{1}{1+e^{-\bm{\theta}^T\bm{x}}}
\end{align*}
Using this probability vector, we predict the label as:
\begin{align*}
y =
\begin{cases}
1 & h_{\theta}\left(\bm{x}\right) \geq 0.5 \\
0 & h_{\theta}\left(\bm{x}\right) < 0.5
\end{cases}
\end{align*}
The above function $g\left(z\right)$ is called the sigmoid function. Also, the threshold here is 0.5, but in some special cases, where one out of precision or recall (later) is more important than the other, the threshold value can be changed to obtain better results.
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{sigmoid_graph}
\caption{Sigmoid function}
\end{figure}

\subsection{Decision Boundary}
It is clear from the graph of the sigmoid function, that it is an increasing function, and it takes a value of 0.5 at $z = 0$. Thus, we predict label:
\begin{align*}
y =
\begin{cases}
1 \text{ if } \bm{\theta}^T\bm{x} \geq 0 \\
0 \text{ if } \bm{\theta}^T\bm{x} < 0
\end{cases}
\end{align*}
This allows us to think of the classification in terms of a decision boundary. This boundary has the equation:
\begin{align*}
\theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n = 0
\end{align*}
The region where the above quantity is positive is the label 1 region, whereas the remaining region is the label 0 region.

\subsection{Cost Function}
In logistic regression, the cross-entropy cost function is used:
\begin{align*}
J\left(\bm{\theta}\right) = -\dfrac{1}{m}\sum_{i=1}^{m}\left[y^{\left(i\right)}\log h_{\theta}\left(\bm{x^{\left(i\right)}}\right) + \left(1 - y^{\left(i\right)}\right)\log \left(1 - h_{\theta}\left(\bm{x^{\left(i\right)}}\right)\right)\right]
\end{align*}
It is clear that the above function adds larger errors for larger deviations in the probability from the true labels. As in linear regression, the cost function is minimized using gradient descent (batch/stochastic/mini-batch). There is no normal equations method here.

\subsection{Gradient Descent}
As in linear regression, gradient descent is implemented by subtracting a multiple of the gradient of the cost function from the parameter vector at each step:
\begin{align*}
\bm{\theta}_{\text{new}} = \bm{\theta} - \alpha
\begin{pmatrix}
\dfrac{\partial}{\partial \theta_0}J\left(\bm{\theta}\right) \\
\dfrac{\partial}{\partial \theta_1}J\left(\bm{\theta}\right) \\
\vdots \\
\dfrac{\partial}{\partial \theta_n}J\left(\bm{\theta}\right) \\
\end{pmatrix}
= \bm{\theta} - \dfrac{\alpha}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(\bm{x^{\left(i\right)}}\right)-y^{\left(i\right)}\right)\bm{x^{\left(i\right)}}
\end{align*}
As can be seen, the gradient expression turns out to be somewhat similar to that in linear regression.

\subsection{Multiclass Classification}
For multiclass classification, the one-vs-rest algorithm is used. In this method, the probability that an instance belongs to class $i$ is calculated for each class $i$, by creating several classifiers (number of classifiers equals the number of classes) and then training each classifier separately. For label prediction, the class label with the maximum probability is returned as the final prediction.

\subsection{Skewed Classes}
There occurs a somewhat funny problem when one of the two classification labels in the data is much less frequent than the other. Suppose we are trying to diagnose cancer in patients, and we achieve $99\%$ classification accuracy (i.e., $1\%$ error) with our model. However, suppose only $0.5\%$ of patients actually have cancer. It is quite disheartening, then, to realize that a model which always predicts absence of cancer would have an accuracy of $99.5\%$, and would beat our model for this accuracy metric. \\
\break
This emphasizes the need for a better error metric for skewed classes. This is where the concepts of precision and recall come in. Let $y = 1$ denote the rare class and $y = 0$ denote the common class. Consider the following table:
\begin{table}[H]
\centering
\begin{tabular}{| c | c | c |}
\hline
\backslashbox{Predicted Class}{Actual Class} & $y=1$ & $y=0$ \\
\hline
$y=1$ & True Positives (TP) & False Positives (FP) \\
$y=0$ & False Negatives (FN) & True Negatives (TN) \\
\hline
\end{tabular}
\end{table}
We define:
\begin{align*}
\text{Precision} &= \text{Fraction of rare class predictions which are actually correct} \\
&= \dfrac{TP}{TP + FP} \\
\text{Recall} &= \text{Fraction of actual rare class instances which are classified correctly} \\
&= \dfrac{TP}{TP + FN}
\end{align*}
We can now evaluate our model using both the above values. To obtain a single evaluation metric, however, the F1 score, which is the harmonic mean of precision and recall, is often used:
\begin{align*}
\text{F1 Score} = \dfrac{2\cdot\text{Precision}\cdot\text{Recall}}{\text{Precision} + \text{Recall}}
\end{align*}
The F1 score is a much better evaluation metric than classification accuracy in case of skewed classes. Its range is 0 to 1. The higher the F1 score, the better is the model.


\section{Overfitting and Underfitting}
A machine learning model learns from patterns observed in the training dataset, and uses the training set observations to make predictions about other data which it hasn't seen before. We have to be careful of this distinction between the training and test datasets. The model only has access to patterns in the training data, and it has to make sure that it only learns patterns which will be present in new datasets as well, and not those patterns which are unique to the training dataset only. However, it cannot be made to learn too less from the training set, since that would make it incapable of accurate predictions. \\
\break
The two problems described above are called overfitting and underfitting respectively:
\begin{enumerate}
\item \textbf{Overfitting}: The model becomes too closely acquainted with observations from the training dataset (often due to too many features), and is unable to generalize to new examples. Such a model is said to have high variance. Overfitting is characterized by very high prediction accuracy on the training set but much worse accuracy on test sets. Overfitting can often be due to very high degree polynomial regression, since this creates a lot of new features.
\item \textbf{Underfitting}: The model is unable to fit well to even the training data itself, and is thus incapable of making accurate predictions on the training set as well as test sets. Such a model is said to have high bias. Underfitting is characterized by low accuracies on training as well as test sets. Underfitting can often be due to very low degree polynomial regression, which does not represent the dataset well.
\end{enumerate}
\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{over_under_fitting}
\caption{Overfitting and underfitting}
\end{figure}

\subsection{Problem Diagnosis by Hypothesis Evaluation}
The problems of overfitting and underfitting can be diagnosed by evaluating the hypothesis using a well defined measure of accuracy of the model. \\
\break
First, we split the available labeled data into three sets; the training dataset, the cross-validation dataset, and the test dataset. The splitting ratio should be close to $60:20:20$. The cross-validation set is used for tuning model parameters to generalize well to new data, and the test set is reserved for final evaluation of the model on new data. \\
\break
Next, the model is trained using the training dataset, via gradient descent, normal equations, or any other method. The training set and cross-validation set prediction errors are then calculated using:
\begin{align*}
J_{train}\left(\bm{\theta}\right) &= \dfrac{1}{2m_{train}}\sum_{i=1}^{m_{train}}\left(h_{\theta}\left(\bm{x^{\left(i\right)}}\right)-y^{\left(i\right)}\right)^2 \\
J_{cv}\left(\bm{\theta}\right) &= \dfrac{1}{2m_{cv}}\sum_{i=1}^{m_{cv}}\left(h_{\theta}\left(\bm{x^{\left(i\right)}}\right)-y^{\left(i\right)}\right)^2
\end{align*}
A high bias (underfit) model is indicated by high values of both $J_{train}\left(\bm{\theta}\right)$ and $J_{cv}\left(\bm{\theta}\right)$ (and close to each other too), whereas a high variance (overfit) model is indicated by a low value of $J_{train}\left(\bm{\theta}\right)$ and a much higher value of $J_{cv}\left(\bm{\theta}\right)$.

\subsection{Regularization: A Solution for Overfitting}
One very commonly used solution for overfitting is regularization. Regularization works well when we have a large number of features.\\
\break
In regularization, we keep all the features but we reduce the magnitudes of parameters $\theta_j$. Mathematically, regularization is done by changing our cost function as follows:
\begin{align*}
J\left(\bm{\theta}\right) = \dfrac{1}{2m}\left[\sum_{i=1}^{m}\left(h_{\theta}\left(\bm{x^{\left(i\right)}}\right)-y^{\left(i\right)}\right)^2 + \lambda\sum_{j=1}^{n}\theta_j^2\right]
\end{align*}
The value of $\lambda$ has to be optimized as well, since it can cause underfitting if it is too high and overfitting if it is too low. Note that we do not penalize the bias parameter $\theta_0$ during regularization.\\
\break
Specifically, in the case of linear regression (and since polynomial regression can be implemented using linear regression, this applies to polynomial regression as well), the modifications caused by regularization are as follows: \\
\break
The cost function becomes:
\begin{align*}
J\left(\bm{\theta}\right) = \dfrac{1}{2m}\left[\sum_{i=1}^{m}\left(\bm{\theta}^T\bm{x^{\left(i\right)}}-y^{\left(i\right)}\right)^2 + \lambda\sum_{j=1}^{n}\theta_j^2\right]
\end{align*}
The gradient descent algorithm is changed as:
\begin{align*}
\left(\theta_0\right)_{new} =& \hspace{1mm} \theta_0 - \dfrac{\alpha}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(\bm{x^{\left(i\right)}}\right)-y^{\left(i\right)}\right) \\
\left(\theta_j\right)_{new} =& \hspace{1mm} \theta_j - \dfrac{\alpha}{m}\left[\left(\sum_{i=1}^{m}\left(h_{\theta}\left(\bm{x^{\left(i\right)}}\right)-y^{\left(i\right)}\right)x^{\left(i\right)}_j\right) + \lambda\theta_j\right] \\
& \hspace{1mm} \forall \hspace{1mm} j \in \{1, 2, \dots n\}
\end{align*}
The normal equations solution is modified as:
\begin{align*}
\bm{\theta} = \left(\bm{X}^T\bm{X} + \lambda
\begin{bmatrix}
    0 & 0 & \dots & 0 \\
    0 & 1 & \dots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \dots & 1
\end{bmatrix}
\right)^{-1}\bm{X}^T\bm{y}
\end{align*}
Similarly, for logistic regression, the cost function becomes:
\begin{align*}
J\left(\bm{\theta}\right) = -\dfrac{1}{m}\sum_{i=1}^{m}\left[y^{\left(i\right)}\log h_{\theta}\left(\bm{x^{\left(i\right)}}\right) + \left(1 - y^{\left(i\right)}\right)\log \left(1 - h_{\theta}\left(\bm{x^{\left(i\right)}}\right)\right)\right] + \dfrac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2
\end{align*}
The gradient descent changes are similar to linear regression, and as before, there is no normal equations method for logistic regression. \\
\break
An important point to note here is that the above changed cost function (in case of regularized learning) is used during the learning process only, and should not be used to evaluate the performance of the model. The model evaluation metric should be a measure of accuracy alone, and should thus be the same as before (i.e., the unregularized cost function).

\subsection{Other Solutions}
To get rid of overfitting or underfitting, the model has to have pretty good tuning of each of its parameters. These include the maximum degree of polynomial features $d$, the number of features $n$, the regularization parameter $\lambda$, the size of the training set $m$, etc. \\
\break
To fine-tune any model parameter, we plot $J_{train}\left(\bm{\theta}\right)$ and $J_{cv}\left(\bm{\theta}\right)$ as functions of that parameter. The trend is as shown in the following plot:
\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{bias_variance_tradeoff}
\caption{The bias-variance trade-off}
\end{figure}
We also know that:
\begin{itemize}
\item Model complexity increases with increase in $d$ and $n$.
\item Model complexity decreases with increase in $\lambda$.
\end{itemize}
Thus, by repeated training and evaluation of the model with changes in a single parameter, and by plotting the error versus parameter curve, we should find a minimum in the cross-validation error. The value of the parameter where this minimum is achieved is the optimum value of that parameter. \\
\break
Another important factor to be considered is the training set size $m$. The curve for $m$ is not the one shown above, since a larger training set is always better than a smaller one. However, we should still know whether a larger dataset would significantly improve our model's accuracy before we go searching for more data. \\
\break
To figure this out, we plot the learning curve of the model. This is the graph of the training and cross-validation errors versus the training set size $m$. For any value of m, we train our model on a subset of our training data having size $m$, and we evaluate it on the smaller training set and the complete cross-validation set. We plot these errors, $J_{train}\left(\bm{\theta}\right)$ and $J_{cv}\left(\bm{\theta}\right)$, as functions of $m$. The learning curve can be of two types (if the model has a problem that needs fixing):
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{learning_curve}
\caption{Learning curves for high bias and high variance models}
\end{figure}
In the first case (high bias, low variance), an increase in training set size wouldn't improve the accuracy much, whereas in the second case (low bias, high variance), an increase in training set size would improve accuracy quite significantly. \\
\break
In a nutshell, debugging of learning algorithms should be performed only after diagnosing bias and variance issues. The steps to be taken to solve the problem are:
\begin{itemize}
\item For high bias (underfitting), try adding additional features and polynomial features, and decrease $\lambda$.
\item For high variance (overfitting), get more training examples, try reducing features, and increase $\lambda$.
\end{itemize}


\section{Support Vector Machines}
A support vector machines (SVM) is a supervised learning model used for classification. It differs from logistic regression in that it exploits the geometrical properties of the data; unlike the statistical approach of logistic regression, it tries to draw the best possible decision boundary for classification, and then uses this boundary for predictions.

\subsection{Hypothesis Function}
The hypothesis function for an SVM is the same as that for logistic regression:
\begin{align*}
h_{\theta}\left(\bm{x}\right) = g\left(\bm{\theta}^T\bm{x}\right) = \dfrac{1}{1+e^{-\bm{\theta}^T\bm{x}}}
\end{align*}

\subsection{Cost Function}
The cost function is modified for an SVM. Here, we want a decision boundary which can distinguish between the two classes quite well, and we wouldn't like to accept a boundary that barely distinguishes data points. For $y=1$, we want $\bm{\theta}^T\bm{x} \gg 0$, and for $y=0$, we want $\bm{\theta}^T\bm{x} \ll 0$. \\
\break
For regularized logistic regression, we had the following cost function:
\begin{align*}
J\left(\bm{\theta}\right) &= -\dfrac{1}{m}\sum_{i=1}^{m}\left[y^{\left(i\right)}\log\left(h_{\theta}\left(\bm{x^{\left(i\right)}}\right)\right) + \left(1 - y^{\left(i\right)}\right)\log \left(1 - h_{\theta}\left(\bm{x^{\left(i\right)}}\right)\right)\right] + \dfrac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2 \\
&= \dfrac{1}{m}\sum_{i=1}^{m}\left[y^{\left(i\right)}\left(-\log\dfrac{1}{1+e^{-\bm{\theta}^T\bm{x^{\left(i\right)}}}}\right) + \left(1 - y^{\left(i\right)}\right)\left(-\log\left(1 - \dfrac{1}{1+e^{-\bm{\theta}^T\bm{x^{\left(i\right)}}}}\right)\right)\right] + \dfrac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2
\end{align*}
For an SVM, we modify the cost function as follows:
\begin{align*}
J\left(\bm{\theta}\right) = C\sum_{i=1}^{m}\left[y^{\left(i\right)}cost_1\left(\bm{\theta}^T\bm{x^{\left(i\right)}}\right) + \left(1 - y^{\left(i\right)}\right)cost_0\left(\bm{\theta}^T\bm{x^{\left(i\right)}}\right)\right] + \dfrac{1}{2}\sum_{j=1}^{n}\theta_j^2
\end{align*}
Here, $C$ represents the inverse of the regularization parameter $\lambda$. The two component cost functions above are:
\begin{align*}
cost_1\left(z\right) &= \max\left(0, 1 - \bm{\theta}^T\bm{x}\right) \\
cost_0\left(z\right) &= \max\left(0, -1 + \bm{\theta}^T\bm{x}\right)
\end{align*}
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{svm_cost_function}
\caption{The two component cost functions of an SVM}
\end{figure}

\subsection{Working of an SVM}
If we ignore $\theta_0$ and assume we are using a large value of $C$, the objective of an SVM can be expressed as follows:
\begin{align*}
&\min_{\bm{\theta}} \hspace{2mm} \dfrac{1}{2}\lVert\bm{\theta}\rVert \\
&\text{s.t. } \hspace{3mm} \bm{\theta}^T\bm{x^{\left(i\right)}} \geq 1 \hspace{1.4cm} \text{ if } y^{\left(i\right)} = 1 \\
&\text{and} \hspace{4mm} \bm{\theta}^T\bm{x^{\left(i\right)}} \leq -1 \hspace{1cm} \text{ if } y^{\left(i\right)} = 0
\end{align*}
Now, $\bm{\theta}^T\bm{x^{\left(i\right)}}$ is the scalar product of the vectors $\bm{\theta}$ and $\bm{x^{\left(i\right)}}$, and is thus also the projection of $\bm{x^{\left(i\right)}}$ along $\bm{\theta}$, multiplied by $\lVert\bm{\theta}\rVert$. Thus, the optimization of the above expression would mean maximizing the projection magnitudes of $\bm{x^{\left(i\right)}}$ along $\bm{\theta}$, so that $\lVert\bm{\theta}\rVert$ can be minimized. Thus, the SVM algorithm finds a hyperplane in the dataset which can clearly distinguish between data point classes, by maximizing the distance of the hyperplane from the data points of both classes.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{svm}
\caption{Hyperplane optimization by an SVM}
\end{figure}

\subsection{SVMs with Kernels}
Kernel functions are used with SVMs to make non-linear decision boundary formation much more effective. A kernel function takes a data point and a landmark as input and returns a new data point. Thus, a dataset can be transformed into another using a kernel function. The most commonly used kernel is the Gaussian kernel, which we will denote by $sim\left(\bm{x}, \bm{y}\right)$, since it is a measure of similarity between the two points $\bm{x}$ and $\bm{y}$.
\begin{align*}
sim\left(\bm{x}, \bm{y}\right) = \exp\left(-\dfrac{\lVert \bm{x}-\bm{y} \rVert^2}{2\sigma^2}\right)
\end{align*}
Given a dataset, let $\bm{l^{(1)}} = \bm{x^{(1)}}$, $\bm{l^{(2)}} = \bm{x^{(2)}}$, \dots, $\bm{l^{(m)}} = \bm{x^{(m)}}$ be landmarks. \\
For each training example $\left(\bm{x^{\left(i\right)}}, y^{\left(i\right)}\right)$:
\begin{align*}
f_0^{\left(i\right)} &= 1 \\
f_1^{\left(i\right)} &= sim\left(\bm{x^{\left(i\right)}}, \bm{l^{\left(1\right)}}\right) \\
f_2^{\left(i\right)} &= sim\left(\bm{x^{\left(i\right)}}, \bm{l^{\left(2\right)}}\right) \\
\vdots & \\
f_m^{\left(i\right)} &= sim\left(\bm{x^{\left(i\right)}}, \bm{l^{\left(m\right)}}\right)
\end{align*}
Thus, we obtain a new dataset $F$ of size $m\times(m+1)$, and we use this to train the model. \\
Our new hypothesis predicts $y^{\left(i\right)} = 1$ if $\bm{\theta}^T\bm{f^{\left(i\right)}} \geq 0$, and $y^{\left(i\right)} = 0$ otherwise. \\
During training too, the cost function should be calculated by a modified SVM cost function, with $\bm{\theta}^T\bm{x^{\left(i\right)}}$ replaced by $\bm{\theta}^T\bm{f^{\left(i\right)}}$. \\
\break
Note that:
\begin{itemize}
\item Large $C$ and small $\sigma$ can cause low bias and high variance.
\item Small $C$ and large $\sigma$ can cause high bias and low variance.
\end{itemize}


\section{K-Means Clustering}
K-means clustering is a clustering algorithm under unsupervised learning. It is an iterative algorithm used to group a dataset into $K$ clusters.

\subsection{Algorithm}
The algorithm works as follows:
\begin{enumerate}
\item Randomly initialize $K$ cluster centroids, $\bm{\mu_1}$, $\bm{\mu_2}$, \dots, $\bm{\mu_K} \in \mathbb{R}^n$.
\item $\forall \hspace{1mm} i \in \{1, \dots, m\}$, $c^{\left(i\right)} := $ index (from 1 to $K$) of the cluster centroid closest to $\bm{x^{\left(i\right)}}$.
\item $\forall \hspace{1mm} k \in \{1, \dots, K\}$, $\bm{\mu_k} := $ centroid (mean) of points assigned to cluster $k$.
\item Repeat steps 2 and 3 above until the cluster centroids stop moving.
\end{enumerate}

\subsection{Optimization Objective}
The optimization objective for K-means clustering can be expressed as follows:
\begin{align*}
&\min_{c^{\left(1\right)}, \dots, c^{\left(m\right)}, \bm{\mu_1}, \dots, \bm{\mu_K}} J\left(c^{\left(1\right)}, \dots, c^{\left(m\right)}, \bm{\mu_1}, \dots, \bm{\mu_K}\right) \\
\text{where } & J\left(c^{\left(1\right)}, \dots, c^{\left(m\right)}, \bm{\mu_1}, \dots, \bm{\mu_K}\right) = \dfrac{1}{m}\sum_{i=1}^{m}\lVert \bm{x^{\left(i\right)}} - \mu_{c^{\left(i\right)}} \rVert^2
\end{align*}
The above cost function is also called the \textit{distortion} of the dataset.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{k_means}
\caption{K-means clustering algorithm}
\end{figure}

\subsection{Random Initialization and Selecting $K$}
The initial cluster centroids should be well spread-out throughout the dataset. To ensure this, the clustering algorithm is performed several times, each time randomly initializing the cluster centroids to be $K$ points from the dataset. The iteration which returns the minimum value of $J\left(c^{\left(1\right)}, \dots, c^{\left(m\right)}, \bm{\mu_1}, \dots, \bm{\mu_K}\right)$ is considered to have the right clusters, and these clusters are finally chosen by the algorithm. \\
\break
As for selecting the number of clusters $K$ for grouping the dataset, this is most often done manually, according to the programmer's requirements, or the nature of the dataset. If unsure about the number of clusters in the data, one can try using the elbow method, although this method does not always work. In this method, the curve of the distortion function (cost function) $J\left(c^{\left(1\right)}, \dots, c^{\left(m\right)}, \bm{\mu_1}, \dots, \bm{\mu_K}\right)$ is plotted against the number of clusters $K$. This graph sometimes has an elbow, a point where the slope of the curve suddenly decreases. The value of $K$ at this elbow is an ideal selection for $K$.


\section{Anomaly Detection}
Unsupervised learning can be used for anomaly detection in data. Anomalies are data points for which one or more feature values lie far away from those of the majority of data instances. Thus, for anomaly detection, we have to come up with a probability function for data points, and we predict an anomaly whenever the probability falls below a threshold value $\epsilon$. 
\subsection{Algorithm}
The following algorithm works on the basis that the feature values distribution should be approximately Gaussian in nature. The algorithm works as follows:
\begin{enumerate}
\item Choose features $\bm{x_i}$ that could be indicative of anomalous instances.
\item Calculate parameters $\mu_1, \dots, \mu_n, \sigma_1^2, \dots, \sigma_n^2$.
\begin{align*}
\mu_j &= \dfrac{1}{m}\sum_{i=1}^{m}\left(x_j^{\left(i\right)}\right)^2 \\
\sigma_j^2 &= \dfrac{1}{m}\sum_{i=1}^{m}\left(x_j^{\left(i\right)} - \mu_j\right)^2
\end{align*}
\item For anomaly detection on a new example $\bm{x}$, calculate:
\begin{align*}
p\left(\bm{x}\right) = \prod_{j=1}^{n}p\left(x_j; \mu_j, \sigma_j^2\right) = \prod_{j=1}^{n}\dfrac{1}{\sqrt{2\pi}\sigma_j}\exp\left(-\dfrac{\left(x_j-\mu_j\right)^2}{2\sigma_j^2}\right)
\end{align*}
Predict an anomaly if $p(\bm{x}) < \epsilon$ for some predefined threshold $\epsilon$.
\end{enumerate}
\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{anomaly_detection}
\caption{An anomaly in a dataset}
\end{figure}

\subsection{Model Evaluation}
An anomaly detection model should not be evaluated using cross-entropy error or prediction accuracy, because it consists of skewed classes. If we have a test dataset with the anomalies already labelled, then we can use it to evaluate our model through metrics like precision, recall, and the F1-score. This is because the anomalous examples are much rarer than the non-anomalous ones, i.e., the problem has skewed classes.

\subsection{Multivariate Gaussian Distribution}
Instead of expressing the probability function as a product of individual feature value probabilities, we could express it as a single multivariate Gaussian probability distribution representing the entire dataset:
\begin{align*}
p\left(\bm{x}\right) = \dfrac{1}{\left(2\pi\right)^{\frac{n}{2}}|\bm{\Sigma}|^{\frac{1}{2}}}\exp\left(-\dfrac{1}{2}\left(\bm{x}-\bm{\mu}\right)^T\bm{\Sigma}^{-1}\left(\bm{x}-\bm{\mu}\right)\right)
\end{align*}
where $\bm{\mu}$ is, as before, the vector of mean feature values (calculated and stored during training), and $\bm{\Sigma}$ is the ($n \times n$) covariance matrix of the mean normalized training dataset:
\begin{align*}
\bm{\Sigma} = \dfrac{1}{m}\sum_{i=1}^{m}\left(\bm{x^{\left(i\right)}} - \bm{\mu}\right)\left(\bm{x^{\left(i\right)}} - \bm{\mu}\right)^T
\end{align*}
Note that the previous probability distribution function (product form) is a special case of the multivariate Gaussian function, with off-diagonal elements of the covariance matrix equal to zero. \\
\break
The multivariate function is advantageous in that it can determine correlations between features by itself for anomaly detection, but it is computationally expensive due to the inverse calculation. Also, to use the multivariate function, we must have $m > n$ to ensure invertibility of $\bm{\Sigma}$.


\section{Recommender Systems}
Recommender systems are programs that explore a user's product ratings, and learn to recommend new products to that user in accordance with his/her preferences. Without loss of generality, we consider the problem of movie recommendations.\\
\break
Suppose we have $n_u$ users and $n_m$ movies. Users watch movies, and rate them after watching. Suppose we have a matrix $\bm{R} = \left[r_{ij}\right]$, where $r_{ij} = 1$ if user $j$ has rated movie $i$ and $0$ otherwise, and a ratings matrix $\bm{Y} = \left[y_{ij}\right]$, where $y_{ij}$ is the rating (from 1 to 5) given by user $j$ to movie $i$, defined only if $r_{ij} = 1$. The way we go about recommending movies to users is by predicting the undefined ratings in matrix $\bm{Y}$, i.e., we guess how users would have rated the movies they haven't watched yet, and then we recommend the movies which get the highest predicted ratings.

\subsection{Content Based Recommendations}
The problem is formulated as follows:
\begin{itemize}
\item[] $r\left(i,j\right) = 1$ if user $j$ has rated movie $i$ (otherwise 0)
\item[] $y^{\left(i,j\right)} =$ rating by user $j$ on movie $i$, if defined
\item[] $\bm{\theta^{\left(j\right)}} =$ parameter vector for user $j$
\item[] $\bm{x^{\left(i\right)}} =$ feature vector for movie $i$
\item[] Predicted rate for user $j$, movie $i = \left(\bm{\theta^{\left(j\right)}}\right)^T\bm{x^{\left(i\right)}}$
\end{itemize}
Thus, the overall optimization objective (for all users) can be expressed as:
\begin{align*}
\min_{\bm{\theta^{\left(1\right)}}, \bm{\theta^{\left(2\right)}}, \dots, \bm{\theta^{\left(n_u\right)}}}\dfrac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}\left(\left(\bm{\theta^{\left(j\right)}}\right)^T\bm{x^{\left(i\right)}} - y^{\left(i,j\right)}\right)^2 + \dfrac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}\left(\theta_k^{(j)}\right)^2
\end{align*}
We can then calculate each user's own parameter vector by minimizing this loss function, and then use these parameters along with the movie features to give predictions. \\
\break
There is one disadvantage with this recommendation method, though. To implement it, we need to set the features (usually the genre distribution) of all the movies manually beforehand. To overcome this problem, the method of collaborative filtering is used.

\subsection{Collaborative Filtering}
Given all $\bm{x^{\left(i\right)}}$, we can learn all $\bm{\theta^{\left(j\right)}}$ as before:
\begin{align*}
\min_{\bm{\theta^{\left(1\right)}}, \bm{\theta^{\left(2\right)}}, \dots, \bm{\theta^{\left(n_u\right)}}}\dfrac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}\left(\left(\bm{\theta^{\left(j\right)}}\right)^T\bm{x^{\left(i\right)}} - y^{\left(i,j\right)}\right)^2 + \dfrac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}\left(\theta_k^{(j)}\right)^2
\end{align*}
Similarly, given all $\bm{\theta^{\left(j\right)}}$, we can learn all $\bm{x^{\left(i\right)}}$ as follows:
\begin{align*}
\min_{\bm{x^{\left(1\right)}}, \bm{x^{\left(2\right)}}, \dots, \bm{x^{\left(n_m\right)}}}\dfrac{1}{2}\sum_{i=1}^{n_m}\sum_{j:r(i,j)=1}\left(\left(\bm{\theta^{\left(j\right)}}\right)^T\bm{x^{\left(i\right)}} - y^{\left(i,j\right)}\right)^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}\left(x_k^{(i)}\right)^2
\end{align*}
In collaborative filtering, we combine both the above into a single optimization objective:
\begin{align*}
\min_{\bm{\theta^{\left(1\right)}}, \dots, \bm{\theta^{\left(n_u\right)}}, \bm{x^{\left(1\right)}}, \dots, \bm{x^{\left(n_m\right)}}} J\left(\bm{\theta^{\left(1\right)}}, \dots, \bm{\theta^{\left(n_u\right)}}, \bm{x^{\left(1\right)}}, \dots, \bm{x^{\left(n_m\right)}}\right)
\end{align*}
where:
\begin{align*}
J\left(\bm{\theta^{\left(1\right)}}, \dots, \bm{\theta^{\left(n_u\right)}}, \bm{x^{\left(1\right)}}, \dots, \bm{x^{\left(n_m\right)}}\right) &= \dfrac{1}{2}\sum_{(i,j):r(i,j)=1}\left(\left(\bm{\theta^{\left(j\right)}}\right)^T\bm{x^{\left(i\right)}} - y^{\left(i,j\right)}\right)^2 \\
& \hspace{2cm} + \dfrac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}\left(x_k^{(i)}\right)^2 + \dfrac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}\left(\theta_k^{(j)}\right)^2
\end{align*}
As always, we can now initialize all $\bm{x^{(i)}}$s and $\bm{\theta^{(j)}}$s to random values and optimize them through gradient descent.\\
\break
There is one problem with this approach. Suppose a user hasn't watched or rated any movies yet. Then, the cost function's first term becomes zero for that user, and the minimization of the second regularization term means that all the user's parameters will be predicted as zero. So, we wouldn't be able to make recommendations. Rather than this, we would practically prefer to recommend movies with high average ratings to that user. To do this, we perform mean normalization of the ratings for each individual movie before running our algorithm; for each movie, we make the mean of our user ratings zero (and store the actual mean). That way, after predicting the rating of a user for a movie, we add back the mean rating before reporting the predicted rating. This solves the new user problem.


\newpage

\topskip0pt
\vspace*{\fill}
\begin{center}
{\huge\scshape Part II \\\vspace{1cm} Neural Networks and Deep Learning}
\end{center}
\vspace*{\fill}

\newpage


\section{Neural Networks}
Although we could use linear and logistic regression algorithms and their variants, or SVMs, to solve machine learning problems quite efficiently, there exist some other learning methods which are much more effective for generating non-linear hypotheses. One such method is neural networks. \\
\break
Neural networks originated when computer scientists tried to mimic the learning processes of the human brain, hence the name. In fact, there is not much in common between the neural networks discussed here and those that constitute our brain. However, the essential notion of neurons receiving several inputs and passing on a single output to the next neuron is preserved in the neural networks which we discuss here.

\subsection{Model Representation}
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{neural_network}
\caption{A deep neural network}
\end{figure}
The above diagram shows a 3-layer neural network (input layer is not really a layer). It has one input layer, two hidden layers, and one output layer. A general neural network has one input layer (with number of input units equal to the number of features available in the dataset), any number of hidden layers (with any number of units each), and an output layer (with number of output units equal to number of values to be predicted; e.g., for a 4-class classification, this number would be 4). \\
\break
We represent the number of units by $L$. Here, $L = 3$. We number the layers from $0$ to $L$; the input layer is called layer $0$, hidden layers are numbered from $1$ to $L-1$ (left to right), and the output layer is called layer $L$. \\
\break
For the layer numbered $l$, we denote the number of units in that layer by $n^{\left[l\right]}$. In the above diagram, we have $n^{\left[0\right]} = 5$, $n^{\left[1\right]} = 7$, $n^{\left[2\right]} = 7$, and $n^{\left[3\right]} = 4$. \\
\break
Each neuron takes input from the previous layer and produces an output, called its activation. We represent the activation of the $k^{\text{th}}$ neuron in layer $l$ by $a_{k}^{\left[l\right]}$. The activations of layer $l$, when stacked in a column vector, form its activations vector, denoted by $\bm{a^{\left[l\right]}}$. Thus, $\bm{a^{\left[0\right]}}$ is the input features vector (for the data instance being considered), and $\bm{a^{\left[L\right]}}$ is the output of the neural network.

\subsection{Forward Propagation}
So, how does the above network take our input and generate an output? Well, each layer in the neural network essentially performs two functions:
\begin{enumerate}
\item Every unit (neuron) in layer $l$ of the neural network first calculates a linear combination of all the values (outputs) from the previous layer $l-1$. For each neuron in layer $l$, this linear combination is calculated by using a different vector of weights for the values from layer $l-1$. When these vectors are stacked as rows of a matrix, we get a weight matrix from layer $l-1$ to layer $l$. This weights matrix is denoted by $\bm{W^{\left[l\right]}}$, and the shape of this matrix is $n^{\left[l\right]} \times n^{\left[l-1\right]}$. The $k^{\text{th}}$ row of the matrix $\bm{W^{\left[l\right]}}$ is the weights vector for calculations for the neuron $a_{k}^{\left[l\right]}$. In addition to these weights, each neuron in each layer also has a bias unit $b_{k}^{\left[l\right]}$ associated with it. Thus, the $n^{\left[l\right]} \times 1$ column vector $\bm{b^{\left[l\right]}}$ is the bias vector for layer $l$ calculation. Mathematically, the first calculation for each layer is:
\begin{align*}
\bm{z^{\left[l\right]}} = \bm{W^{\left[l\right]}}\bm{a^{\left[l-1\right]}} + \bm{b^{\left[l\right]}}
\end{align*}
\item Secondly, each neuron in the layer applies an activation function (same function for all neurons in the same layer) to the previously computed linear combination $z_k^{\left[l\right]}$. This is necessary for the network to be able to generate non-linear hypotheses, and an extremely effective way of doing it too. The activation function is generally taken to be the ReLU (Rectified Linear Unit) function for hidden layers, the sigmoid function for the output layer if it is a classification task (to be able to generate a probability output between 0 and 1), and the identity function ($g\left(z\right) = z$) for the output layer if it is a regression task (to be able to generate any real number as an output).
\begin{align*}
\text{ReLU function: } & g\left(z\right) = \max\left(0, z\right) \\
\text{Sigmoid function: } & g\left(z\right) = \dfrac{1}{1+e^{-z}}
\end{align*}
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{sigmoid_vs_relu}
\caption{Sigmoid and ReLU functions}
\end{figure}
Thus, we obtain the activations of layer $l$ as:
\begin{align*}
\bm{a^{\left[l\right]}} = g\left(\bm{z^{\left[l\right]}}\right)
\end{align*}
\end{enumerate}
In the above discussion, we have till now considered manipulating only a single data instance. However, we can further vectorize the calculations to use the entire dataset together. \\
\break
Let $\bm{A^{\left[l\right]}}$ be the matrix of activations for layer $l$. In this matrix, the $i^{\text{th}}$ column contains the activations for the $i^{\text{th}}$ data instance. In accordance with this convention, we now have to send in our input dataset with a shape of $n_x \times m$ (unlike we used to do in machine learning); i.e., each input data instance is contained in a column. Thus, the above two calculations are changed to:
\begin{align*}
\bm{Z^{\left[l\right]}} &= \bm{W^{\left[l\right]}}\bm{A^{\left[l-1\right]}} + \bm{b^{\left[l\right]}} \\
\bm{A^{\left[l\right]}} &= g\left(\bm{Z^{\left[l\right]}}\right)
\end{align*}
It is very useful in debugging to keep checking shapes of matrices. Here, $\bm{Z^{\left[l\right]}}$ and $\bm{A^{\left[l\right]}}$ both have shape $n^{\left[l\right]} \times m$, $\bm{W^{\left[l\right]}}$ has shape $n^{\left[l\right]} \times n^{\left[l-1\right]}$, and $\bm{b^{\left[l\right]}}$ has shape $n^{\left[l\right]} \times 1$. Also note that the bias vector $\bm{b^{\left[l\right]}}$ is added separately to each column of the matrix $\bm{W^{\left[l\right]}}\bm{A^{\left[l-1\right]}}$; the above written equation assumes broadcasting to be understood. \\
\break
We have now discussed how a layer calculates its outputs using outputs from the previous layer along with weights and biases. Looking at the big picture, we first assign our input dataset to the matrix $\bm{A^{\left[0\right]}}$, and using the above two equations, with the weights, biases, and activation function of each layer, the neural network propagates the data forward and finally generates an output in the fom of $\bm{A^{\left[L\right]}}$. This is called forward propagation. If this is the training phase, the output of forward propagation is checked for accuracy and then gradient descent is used to improve the weights and biases of the network. If training is over, and the parameters have been optimized, then this final output is our prediction.

\subsection{Network Training and Backward Propagation}
How do we train a neural network? We have a training dataset $\bm{X}$, and we have a network model like the figure shown previously, but how do we optimize the parameters for maximum prediction accuracy? The following steps are used:
\begin{enumerate}
\item We first initialize all the network parameters (weights and biases) to random values (but keep them small for faster gradient descent learning). It is important to know that we cannot initialize everything to zero, because this would generate symmetry between all neurons, and would not work at all. Generally, we initialize weights to small random values ($\sim 10^{-2}$) and biases to zero. We also set $\bm{A^{\left[0\right]}}$ to be the training dataset $\bm{X}$.
\item We then use forward propagation to move through the neural network and finally generate some outputs in layer $L$. During forward propagation, the outputs of each layer are stored in a cache, to be used during gradient descent. The final outputs are predictions, and using a cost function (such as mean squared error, or binary/categorical cross-entropy), we calculate the error in our predictions.
\item We use backward propagation to calculate the gradients of the cost function with respect to the model parameters. We use $dp$ to denote the derivative of the cost function with respect to a parameter $p$. The calculations are as follows:
\begin{align*}
\bm{dZ^{\left[L\right]}} &= \dfrac{\partial J}{\partial \bm{Z^{\left[L\right]}}} & \text{(depends on cost function)} \\
&= \bm{A^{\left[L\right]}} - \bm{Y} & \text{(for the cross-entropy cost function)} \\
\bm{dW^{\left[L\right]}} &= \dfrac{1}{m}\bm{dZ^{\left[L\right]}}\left(\bm{A^{\left[L-1\right]}}\right)^T &\\
\bm{db^{\left[L\right]}} &= \dfrac{1}{m}\bm{dZ^{\left[L\right]}}
\begin{bmatrix}
1 \\ 1 \\ \vdots \\ 1
\end{bmatrix}
& \text{(avg of all columns of the matrix)} \\
&= np.sum(\bm{dZ^{\left[L\right]}}, axis=1, keepdims=True)/m & \text{(in NumPy)} \\
\bm{dZ^{\left[L-1\right]}} &= \left(\bm{W^{\left[L\right]}}\right)^T\bm{dZ^{\left[L\right]}} * \left(g^{[L-1]}\right)'\left(\bm{Z^{\left[L-1\right]}}\right) &
\end{align*}
and so on until we reach $\bm{dW^{\left[1\right]}}$ and $\bm{db^{\left[1\right]}}$.
\item Now that we have the gradients, we perform gradient descent as before. Given a learning rate $\alpha$, we reassign (for $l$ from 1 to $L$):
\begin{align*}
\bm{W^{\left[l\right]}} &:= \bm{W^{\left[l\right]}} - \alpha\bm{dW^{\left[l\right]}} \\
\bm{b^{\left[l\right]}} &:= \bm{b^{\left[l\right]}} - \alpha\bm{db^{\left[l\right]}}
\end{align*}
\item Thus, we have achieved one iteration of gradient descent. We perform steps 2-4 as many times as the pre-decided number of iterations, and finally obtain the optimized weights and biases for the neural network.
\end{enumerate}
Once we've trained the network, our model is ready for making predictions! Its accuracy can be checked by applying the same cost function (which was used for training) on its predictions.
\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{neural_network_training}
\caption{The process of training a neural network}
\end{figure}
\newpage

\subsection{Mathematics of the Backpropagation Algorithm}
In the previous section, we saw some gradient formulae, for implementing backpropagation algorithm. In this section, I attempt to derive those formulae by using the formulae already defined for forward propagation. \\
\break
Here, we consider only one training instance. The derivation can be easily extended to cover $m$ instances; weights and biases gradients just have to be averaged over all the training examples. Also, to make the neural network general, we won't impose restrictions on $n^{[L]}$; it may be 1 or greater. However, we will assume that the last activation is sigmoid. From forward propagation, we have for $l \in \{1, 2, \dots, L\}$:
\begin{align*}
\bm{a^{\left[0\right]}} &= \bm{x} \\
\bm{z^{\left[l\right]}} &= \bm{W^{\left[l\right]}}\bm{a^{\left[l-1\right]}} + \bm{b^{\left[l\right]}} \\
\bm{a^{\left[l\right]}} &= g^{[l]}\left(\bm{z^{\left[l\right]}}\right)
\end{align*}
At the end of forward propagation, we have our loss function:
\begin{align*}
\bm{J} = -\sum_{k=1}^{n^{[L]}}\left[y_{k}\log\left(a_{k}^{[L]}\right) + \left(1-y_{k}\right)\log\left(1-a_{k}^{[L]}\right)\right]
\end{align*}
We first calculate the derivative of $\bm{J}$ w.r.t. $\bm{z^{\left[L\right]}}$, denoted by $\bm{dz^{\left[L\right]}}$:
\begin{align*}
\bm{dz^{\left[L\right]}} &= \dfrac{\partial \bm{J}}{\partial \bm{z^{\left[L\right]}}} = 
\begin{bmatrix}
\dfrac{\partial \bm{J}}{\partial z_{1}^{\left[L\right]}} &
\dfrac{\partial \bm{J}}{\partial z_{2}^{\left[L\right]}} &
\dots &
\dfrac{\partial \bm{J}}{\partial z_{n^{[L]}}^{\left[L\right]}}
\end{bmatrix}^T
\end{align*}
Also:
\begin{align*}
\dfrac{\partial \bm{J}}{\partial z_k^{\left[L\right]}} &= \dfrac{\partial \bm{J}}{\partial a_{k}^{\left[L\right]}}\cdot\dfrac{\partial a_{k}^{\left[L\right]}}{\partial z_{k}^{\left[L\right]}} \\
&= \left(\dfrac{1-y_{k}}{1-a_{k}^{[L]}} - \dfrac{y_{k}}{a_{k}^{[L]}}\right)\sigma'\left(z_{k}^{\left[L\right]}\right) \\
&= \dfrac{a_k^{[L]} - y_k}{a_k^{[L]}\left(1-a_k^{[L]}\right)}\cdot a_k^{[L]}\left(1-a_k^{[L]}\right) \\
&= a_k^{[L]} - y_k
\end{align*}
Therefore:
\begin{align}
\bm{dz^{\left[L\right]}} = \bm{a^{[L]}} - \bm{y}
\end{align}
Thus, we now have $\bm{dz^{\left[L\right]}}$, the gradient for the last layer. Next, we will see how, given $\bm{dz^{\left[l\right]}}$ and parameters, we can compute $\bm{dW^{\left[l\right]}}$, $\bm{db^{\left[l\right]}}$, and $\bm{dz^{\left[l-1\right]}}$, for any general $l \in \{1, 2, \dots, L\}$.
We now calculate the derivative of $\bm{J}$ w.r.t. $\bm{W^{\left[l\right]}}$, denoted by $\bm{dW^{\left[l\right]}}$:
\begin{align*}
\bm{dW^{\left[l\right]}} &= \dfrac{\partial \bm{J}}{\partial \bm{W^{\left[l\right]}}} = 
\begin{bmatrix}
\dfrac{\partial \bm{J}}{\partial W_{11}^{\left[l\right]}} &
\dfrac{\partial \bm{J}}{\partial W_{12}^{\left[l\right]}} &
\dots &
\dfrac{\partial \bm{J}}{\partial W_{1n^{[l-1]}}^{\left[l\right]}} \\
\dfrac{\partial \bm{J}}{\partial W_{21}^{\left[l\right]}} &
\dfrac{\partial \bm{J}}{\partial W_{22}^{\left[l\right]}} &
\dots &
\dfrac{\partial \bm{J}}{\partial W_{2n^{[l-1]}}^{\left[l\right]}} \\
\vdots & \vdots & \ddots & \vdots \\
\dfrac{\partial \bm{J}}{\partial W_{n^{[l]}1}^{\left[l\right]}} &
\dfrac{\partial \bm{J}}{\partial W_{n^{[l]}2}^{\left[l\right]}} &
\dots &
\dfrac{\partial \bm{J}}{\partial W_{n^{[l]}n^{[l-1]}}^{\left[l\right]}} 
\end{bmatrix}
\end{align*}
Also:
\begin{align*}
\dfrac{\partial \bm{J}}{\partial W_{jk}^{\left[l\right]}} &= \dfrac{\partial \bm{J}}{\partial z_j^{\left[l\right]}}\cdot\dfrac{\partial z_j^{\left[l\right]}}{\partial W_{jk}^{\left[l\right]}} \\
&= \left(dz_{j}^{\left[l\right]}\right)\cdot\left(a_{k}^{[l-1]}\right)
\end{align*}
Therefore:
\begin{align}
\bm{dW^{\left[l\right]}} = \bm{dz^{\left[l\right]}}(\bm{a^{[l-1]}})^T
\end{align}
Next, we compute the derivative of $\bm{J}$ w.r.t. $\bm{b^{\left[l\right]}}$, denoted by $\bm{db^{\left[l\right]}}$:
\begin{align*}
\bm{db^{\left[l\right]}} = \dfrac{\partial \bm{J}}{\partial \bm{b^{\left[l\right]}}} = 
\begin{bmatrix}
\dfrac{\partial \bm{J}}{\partial b_{1}^{\left[l\right]}} &
\dfrac{\partial \bm{J}}{\partial b_{2}^{\left[l\right]}} &
\dots &
\dfrac{\partial \bm{J}}{\partial b_{n^{[l]}}^{\left[l\right]}}
\end{bmatrix}^T
\end{align*}
Also:
\begin{align*}
\dfrac{\partial \bm{J}}{\partial b_k^{\left[l\right]}} &= \dfrac{\partial \bm{J}}{\partial z_k^{\left[l\right]}}\cdot\dfrac{\partial z_k^{\left[l\right]}}{\partial b_k^{\left[l\right]}} \\
&= \left(dz_{j}^{\left[l\right]}\right)\cdot 1
\end{align*}
Therefore:
\begin{align}
\bm{db^{\left[l\right]}} = \bm{dz^{\left[l\right]}}
\end{align}
Next, we compute the derivative of $\bm{J}$ w.r.t. $\bm{z^{\left[l-1\right]}}$, denoted by $\bm{dz^{\left[l-1\right]}}$:
\begin{align*}
\bm{dz^{\left[l-1\right]}} = \dfrac{\partial \bm{J}}{\partial \bm{z^{\left[l-1\right]}}} = 
\begin{bmatrix}
\dfrac{\partial \bm{J}}{\partial z_{1}^{\left[l-1\right]}} &
\dfrac{\partial \bm{J}}{\partial z_{2}^{\left[l-1\right]}} &
\dots &
\dfrac{\partial \bm{J}}{\partial z_{n^{[l-1]}}^{\left[l-1\right]}}
\end{bmatrix}^T
\end{align*}
Also:
\begin{align*}
\dfrac{\partial \bm{J}}{\partial z_k^{\left[l-1\right]}} &= \sum_{j=1}^{n^{[L]}}\dfrac{\partial \bm{J}}{\partial z_j^{\left[l\right]}}\cdot\dfrac{\partial z_j^{\left[l\right]}}{\partial a_k^{\left[l-1\right]}}\cdot\dfrac{\partial a_k^{\left[l-1\right]}}{\partial z_k^{\left[l-1\right]}} \\
&= \sum_{j=1}^{n^{[L]}}\left(dz_{j}^{\left[l\right]}\right)\cdot\left(W_{jk}^{\left[l\right]}\right)\cdot\left(\left(g^{[l-1]}\right)'\left(z_k^{\left[l-1\right]}\right)\right)
\end{align*}
Therefore:
\begin{align}
\bm{dz^{\left[l-1\right]}} = \left(\bm{W^{\left[l\right]}}\right)^T\bm{dz^{\left[l\right]}} * \left(g^{[l-1]}\right)'\left(\bm{z^{\left[l-1\right]}}\right)
\end{align}
Here, $*$ stands for element-wise multiplication of matrices. \\
Hence, equation (1) acts as an induction base, and with equations (2), (3), and (4), we can say by induction that we can calculate the gradient of $\bm{J}$ w.r.t. all weights and biases of the entire neural network. We can then subtract these gradients (multiplied by a learning rate) from the respective parameters to make $\bm{J}$ smaller and smaller, which is essentially what we discussed previously.


\newpage

\topskip0pt
\vspace*{\fill}
\begin{center}
{\huge\scshape Part III \\\vspace{1cm} Project Details}
\end{center}
\vspace*{\fill}

\newpage

\section{Updated Plan of Action}
\vspace{0.5cm}
Following is my updated plan of action for SoS 2020: \\
\vspace{0.5cm}\\
\begin{tabular}{p{4.6cm}|p{8.8cm}}
Week 1 (May 2 - May 8)
&
Hyperparameter tuning, regularization, optimization of neural networks, structuring machine learning projects.
\\
[3mm]
Week 2 (May 9 - May 15)
&
Convolutional neural networks, sequence models, image classification, loss functions and optimization, training neural networks for CV.
\\
[3mm]
Week 3 (May 16 - May 22)
&
Deep learning software, CNN architectures, recurrent neural networks, detection and segmentation, visualizing and understanding features.
\\
[3mm]
Week 4 (May 23 - May 29)
&
Generative models, reinforcement learning, deep reinforcement learning.
\\
[3mm]
Week 5 (May 30 - Jun 5)
&
Efficient methods and hardware for deep learning, adversarial examples and adversarial training.
\\
[3mm]
Week 6 (Jun 6 - Jun 10)
&
Endterm report writing and final submission.
\\
\end{tabular}
\vspace{2cm}

\section{Resources}
\begin{enumerate}
\item Andrew Ng's Machine Learning Course on Coursera
\item Andrew Ng's Deep Learning Specialization on Coursera
\item Stanford University's CS231n Course Material
\item 'Deep Learning' by MIT Press
\item 'Pattern Recognition and Machine Learning' by Christopher Bishop
\end{enumerate}

\end{document}